---
title: "반복법을 이용한 선형 시스템의 해를 구하기"
language: ../_language-ko_custom.yml 

number-sections: true
number-depth: 2
crossref:
  chapters: false
---



## 스펙트럼 반경과 반복법을 통한 선형시스템의 해


### 스펙트럼 반경

우리는 앞서 직접적으로 선형 시스템 $\boldsymbol{Ax}=\boldsymbol{b}$ 의 해를 구하였다. 그러나 선형 시스템이 매우 크며, $\boldsymbol{A}$ 의 대분이 $0$ 일 경우는 직접적으로 구하지 않고 반복법을 통해 구하는 방법이 더 빠르다. 여기서는 이를 알아보기로 한다. 먼저 많이 사용하게 될 개념을 우선 정하기로 하자.

</br>

::: {.callout-note appearance="simple" icon="false"}

::: {#def-spectra_radius}

#### 스펙트럼 반경(spectral radius)
&emsp; ($1$) 정사각 행렬 $\boldsymbol{A}$ 에 대해 $\lambda(\boldsymbol{A})$ 는 $\boldsymbol{A}$ 의 모든 고유값들의 집합이다.

&emsp; ($2$) 행렬 $\boldsymbol{A}$ 의 스펙트럼 반경 $\rho(\boldsymbol{A})$ 는 $\lambda(\boldsymbol{A})$ 의 절대값의 상한으로 정의된다. 즉,

$$
\rho (\boldsymbol{A}) := \max \{ |\lambda| : \lambda \in \lambda(\boldsymbol{A})\}.
$$

:::
:::

</br>

$\rho(\boldsymbol{A})$ 는 행렬에 대해 고유하게 정해지는 값이지만 행렬의 노름이 될 수 없다. 예를 들어 

$$
\boldsymbol{A} = \begin{bmatrix}2 & 1\\  2 &-2\end{bmatrix},\, \boldsymbol{B} = \begin{bmatrix} 2 & 4 \\ 1 &-3\end{bmatrix}
$$

의 경우 $\rho(\boldsymbol{A}) \approx 2.45$, $\rho(\boldsymbol{B}) \approx 3.70$, $\rho(\boldsymbol{A}+\boldsymbol{B}) \approx 6.44$ 이므로 $\rho(\boldsymbol{A}+\boldsymbol{B}) > \rho(\boldsymbol{A}) + \rho (\boldsymbol{B})$ 인데 이것은 행렬의 노름이 될 조건에 위배된다.

</br>

::: {#prp-spectral_radius} 

$\boldsymbol{A}\in \mathbb{F}^{n \times n}$ 에 대해 

&emsp; ($1$) $\|\boldsymbol{A}\|_2 = \left[\rho (\boldsymbol{A}^T\boldsymbol{A})\right]^{1/2}$ 이며,


&emsp; ($2$) $\rho(\boldsymbol{A})$ 는 모든 자연스러운 노름에 대해  $\|\boldsymbol{A}\|$ 보다 작거나 같다. 
:::

::: {.proof}
($1$) 증명은 Isacson, Keller 의 *Analysis of numerical method* (1966) 을 참고하라

($2$) $\lambda \in \lambda(\boldsymbol{A})$ 이고 $\boldsymbol{x}\in \mathbb{F}^n$ 에 대해 $\|\boldsymbol{x}\|=1$ 일 때, 

$$
|\lambda| =  \|\lambda \boldsymbol{x}\| = \|\boldsymbol{Ax}\| \le \|\boldsymbol{A}\| \|\boldsymbol{x}\| = \|\boldsymbol{A}\|
$$

:::

</br>

::: {.callout-note appearance="simple" icon="false"}

::: {#def-convergent_matrix}

#### 행렬의 수렴

정사각 행렬 $\boldsymbol{A}\in \mathbb{F}^{n \times n}$ 이 다음 조건을 만족할 때 행렬 $\boldsymbol{A}$ 가 수렴한다고 한다.

$$
\lim_{k \to \infty} \boldsymbol{A}^k = \boldsymbol{0}.
$$
:::
:::

</br>

::: {#prp-convergent_matrix}

정사각 행렬 $\boldsymbol{A}\in \mathbb{F}^{n \times n}$ 에 대해 다음은 동치이다.

&emsp; ($1$) $\boldsymbol{A}$ 는 수렴한다.

&emsp; ($2$) 자연스러운 행렬 노름에 대해 $\displaystyle \lim_{k \to \infty} \left\|\boldsymbol{A}^k\right\|=\boldsymbol{0}$ 이다.

&emsp; ($3$) $\rho (\boldsymbol{A})< 1$.

&emsp; ($3$) $\displaystyle \lim_{k\to \infty} \boldsymbol{A}^k \boldsymbol{x} = \boldsymbol{0}$. 

:::

::: {.proof}
증명은 Isacson, Keller 의 *Analysis of numerical method* (1966) 을 참고하라
::: 


</br>

선형 시스템 $\boldsymbol{Ax}=\boldsymbol{b}$ 를 반복법으로 구하는 방법중에 가장 일반적으로 사용되는 방법은 주어진 선형 방정식을 다음의 형태로 변형하는데서 시작한다. 

$$
\boldsymbol{x} = \boldsymbol{Tx}+ \boldsymbol{c}.
$$

만약 $\boldsymbol{T}$ 가 어떤 조건을 만족하면 초기값 $\boldsymbol{x}^{(0)}$ 부터 시작하여 
$$
\boldsymbol{x}^{(k+1)} =  \boldsymbol{Tx}^{(k)}  + \boldsymbol{c}
$$

에서 생성되는 수열 $\langle \boldsymbol{x}^{(k)}\rangle$, $k=0,\,1,\ldots$ 는 $\boldsymbol{x} = \boldsymbol{Tx}+ \boldsymbol{c}$ 를 만족하는 $\boldsymbol{x}$ 로 수렴한다. 

</br>

::: {#lem-iterative_method_1}

행렬 $\boldsymbol{T}\in \mathbb{F}^{n \times n}$ 의 스펙트럼 반경이 $1$ 보다 작다면 $\boldsymbol{I}-\boldsymbol{T}$ 는 가역행렬이며,

$$
(\boldsymbol{I}-\boldsymbol{T})^{-1} = \sum_{j=0}^{\infty} \boldsymbol{T}^j
$$

이다.

:::

::: {.proof}

$\boldsymbol{x}$ 가 $\boldsymbol{T}$ 의 고유벡터이며 그 고유값이 $\lambda$ 인것과 $\boldsymbol{x}$ 가 $\boldsymbol{I}-\boldsymbol{T}$ 의 공유벡터이며 그 고유값이 $(1-\lambda)$ 인것은 동치이다. 그런데 $\boldsymbol{T}$ 의 고유값의 절대값이 $1$ 보다 작기 때문에 0 은 $(\boldsymbol{I}-\boldsymbol{T})$ 의 고유값이 될 수 없으며, 따라서 $\boldsymbol{I}-\boldsymbol{T}$ 는 가역행렬이다.

또한 

$$
(\boldsymbol{I}-\boldsymbol{T})\left(\sum_{j=0}^N \boldsymbol{T}^j\right) = \boldsymbol{I} -\boldsymbol{T}^{N+1}
$$


이므로 @prp-convergent_matrix 에 의해 $(\boldsymbol{I}-\boldsymbol{T})^{-1} = \displaystyle\sum_{j=0}^\infty \boldsymbol{T}^j$ 이다. $\square$

:::

</br>

::: {#thm-iterative_method}

임의의 $\boldsymbol{x}^{(0)},\, \boldsymbol{c}\in \mathbb{F}^{n}$ 과 행렬 $\boldsymbol{T}\in \mathbb{F}^{n \times n}$ 에 대해 

$$
\boldsymbol{x}^{(k+1)} =  \boldsymbol{Tx}^{(k)}+ \boldsymbol{c}
$$

에 의해 생성되는 수열 $\langle \boldsymbol{x}^{k}\rangle$ 이 $\boldsymbol{x}=\boldsymbol{Tx}+\boldsymbol{c}$ 를 만족하는 유일한 $\boldsymbol{x}$ 로 수렴할 필요충분조건은 $\rho(\boldsymbol{T})<1$ 이다.

:::

::: {.proof}

$\rho(\boldsymbol{T})<1$ 임을 가정하자. 

$$
\begin{aligned}
\boldsymbol{x}^{(k+1)} &= \boldsymbol{Tx}^{k}+\boldsymbol{c} \\
&= \boldsymbol{T}\left(\boldsymbol{Tx}^{k-1} + \boldsymbol{c}\right) + \boldsymbol{c}  \\
&\qquad \qquad \vdots \\
&=  \boldsymbol{T}^{k+1}\boldsymbol{x}^{(0)} + (\boldsymbol{T}^{k}+ \cdots + \boldsymbol{T}+ \boldsymbol{I})\boldsymbol{c}
\end{aligned} 
$$

이므로 $k \to \infty$ 극한에서 $\boldsymbol{x}^{(\infty)} = \left(\boldsymbol{I}-\boldsymbol{T}\right)^{-1}\boldsymbol{c}$ 이므로, $\boldsymbol{x}$ 는 수렴하며 $\boldsymbol{x}^{(\infty)} = \boldsymbol{Tx}^{(\infty)} +\boldsymbol{c}$ 를 만족한다. 

이제 $\displaystyle \lim_{k \to \infty}\boldsymbol{x}^{(k)} = \boldsymbol{x}$ 이며 $\boldsymbol{x}$ 가 $\boldsymbol{x}=\boldsymbol{Tx}+\boldsymbol{c}$ 를 만족하는 유일한 해라고 하자. 즉 $\boldsymbol{I}-\boldsymbol{T}$ 는 가역이다. 임의의 $\boldsymbol{z}\in \mathbb{F}^n$ 에 대해 $\boldsymbol{x}^{(0)} = \boldsymbol{x}-\boldsymbol{z}$ 라고 하자. $\boldsymbol{x}^{(k+1)} = \boldsymbol{Tx}^{(k)} + \boldsymbol{c}$ 이면 $\langle \boldsymbol{x}^{k}\rangle$ 은 $\boldsymbol{x}$ 로 수렴하며,

$$
\boldsymbol{x}-\boldsymbol{x}^{(k+1)} = (\boldsymbol{Tx}+\boldsymbol{c})-(\boldsymbol{Tx}^{k}+\boldsymbol{c}) =  \boldsymbol{T}(\boldsymbol{x}-\boldsymbol{x}^{(k)})
$$

이므로, 

$$
\boldsymbol{x}-\boldsymbol{x}^{(k+1)} = \boldsymbol{T}^{k+1}(\boldsymbol{x}-\boldsymbol{x}^{(0)}) = \boldsymbol{T}^{(k+1)}\boldsymbol{z}
$$

이다. @prp-convergent_matrix 에 따라 임의의 $\boldsymbol{z}$ 에 대해 $\displaystyle \lim_{k \to \infty} \boldsymbol{T}^{(k+1)}\boldsymbol{z}=0$ 이면 $\rho(\boldsymbol{T})<1$ 이다. $\square$

:::

</br>

::: {#cor-iterative_method}

임의의 자연스러운 행렬 노름에 대해 $\|\boldsymbol{T}\|<1$ 이면 임의의 벡터 $\boldsymbol{c}$ 에 대해 $\boldsymbol{x}^{(k+1)} = \boldsymbol{Tx}^{(k)}+\boldsymbol{c}$ 에 의해 생성되는 수열 $\langle \boldsymbol{x}^{(k)} \rangle$ 은 초기값 $\boldsymbol{x}^{(0)}$ 에 무관하게 $\boldsymbol{x}=\boldsymbol{Tx} + \boldsymbol{c}$ 를 만족하는 $\boldsymbol{x}$ 로 수렴한다. 또한 다음을 만족한다.

&emsp; ($1$) $\| \boldsymbol{x}-\boldsymbol{x}^{(k)}\| \le \|\boldsymbol{T}\|^k \|\boldsymbol{x}-\boldsymbol{x}^{(0)}\|$,

&emsp; ($2$) $\displaystyle \|\boldsymbol{x}-\boldsymbol{x}^{(k)}\| \le \dfrac{\|\boldsymbol{T}\|^k}{1-\|\boldsymbol{T}\|} \|\boldsymbol{x}^{(1)}-\boldsymbol{x}^{(0)}\|$.
:::

::: {.proof}

$\langle \boldsymbol{x}^{(k)} \rangle$ 의 수렴은 @thm-iterative_method 로 부터 알 수 있다. @prp-spectral_radius 에 의해 $\rho(\boldsymbol{T}) \le \|\boldsymbol{T}\|<1$ 이다. 따라서 

$$
\|\boldsymbol{x}-\boldsymbol{x}^{(k)}\| \le \|\boldsymbol{T}\| \cdot \|\boldsymbol{x}-\boldsymbol{x}^{(k-1)}\| \le  \cdots \le \|\boldsymbol{T}\|^{k} \cdot \|\boldsymbol{x}-\boldsymbol{x}^{(0)}\|
$$

이다. 

또한

$$
\begin{aligned}
\|\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}\| & = \|\boldsymbol{Tx}^{(k)}- \boldsymbol{Tx}^{(k-1)}\|\le \|\boldsymbol{T}\|\cdot \| \boldsymbol{x}^{(k)}-\boldsymbol{x}^{(k-1)} \| \\
& \le \cdots \le \|\boldsymbol{T}\|^k \cdot \|\boldsymbol{x}^{(1)}-\boldsymbol{x}^{(0)}\|
\end{aligned}
$$

이므로 양의 정수 $m$ 에 대해

$$
\begin{aligned}
\|\boldsymbol{x}^{(k+m)} -\boldsymbol{x}^{(k)}\| & = \| \boldsymbol{x}^{(k+m)} - \boldsymbol{x}^{(k+m-1)} + \boldsymbol{x}^{(k+m-1)} - \cdots + \boldsymbol{x}^{(k+1)}- \boldsymbol{x}^{(k)} \| \\
&\le  \| \boldsymbol{x}^{(k+m)} - \boldsymbol{x}^{(k+m-1)} \| + \cdots +  \| \boldsymbol{x}^{(k+1)} - \boldsymbol{x}^{(k)}\| \\
&\le \|\boldsymbol{T}\|^{k+m-1} \cdot \|\boldsymbol{x}^{(1)} - \boldsymbol{x}^{(0)}\| + \cdots +\|\boldsymbol{T}\|^k \|\boldsymbol{x}^{(1)} - \boldsymbol{x}^{(0)}\|  \\
& = \|\boldsymbol{T}\|^k \left(1 + \|\boldsymbol{T}\| + \cdots +\|\boldsymbol{T}\|^{m-1}\right) \|\boldsymbol{x}^{(1)} - \boldsymbol{x}^{(0)}\| \\
\end{aligned}
$$

이므로 $m \to \infty$ 극한에서, 

$$
\|\boldsymbol{x}-\boldsymbol{x}^{(k)} \| \le \dfrac{\|\boldsymbol{T}\|^{k}}{1-\|\boldsymbol{T}\|} \|\boldsymbol{x}^{(1)}-\boldsymbol{x}^{(0)}\|
$$

이다.

:::

</br>


## 야코비(Jacobi) 방법과 가우스-지델(Gauss-Siedel) 방법

일반적으로 반복법에 의해 선형방정식 $\boldsymbol{Ax}=\boldsymbol{b}$ 를 푸는 것은, 이 선형방정식을 변형하여

$$
\boldsymbol{x} = \boldsymbol{Tx}+\boldsymbol{c}
$$

의 꼴로 만드는 데서 시작한다. $k$ 번째 반복에 의해 얻은 $\boldsymbol{x}$ 를 $\boldsymbol{x}^{(k)}$ 라고 할 때 $\boldsymbol{x}^{(k+1)} = \boldsymbol{Tx}^{(k)} +\boldsymbol{c}$ 를 얻으며 $\boldsymbol{x}^{(k+1)}$ 과 $\boldsymbol{x}^{(k)}$ 의 차가 어느 기준 이하일 때 답을 얻은것으로 한다. 보통은 어떤 정해진 노름과 값 $\varepsilon>0$ 에 대해 $\|\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}\|<\varepsilon$ 혹은 $\dfrac{\|\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}\|}{\|\boldsymbol{x}^{(k+1)}\|} < \varepsilon$ 일 때이며 이때의 기준값을 **tolerence** 라고 한다. Tolerance 를 계산할때는 보통 $L_\infty$ 노름을 사용한다.

</br>

### 야코비 방법

$\boldsymbol{A}\in \mathcal{M}_{n \times n}(\mathbb{R})$ 에 대해 $\boldsymbol{D}$ 를 $\boldsymbol{A}$ 의 대각행렬, $\boldsymbol{L}$ 과 $\boldsymbol{U}$ 를 각각 $\boldsymbol{A}$ 의 하삼각 행렬부분과 상삼각 행렬에서 대각성분을 $0$ 으로 만든 행렬에 $-1$ 을 곱한 행렬이라고 하자. 즉 $\boldsymbol{A}=\boldsymbol{D}-\boldsymbol{L}-\boldsymbol{U}$ 이며, $\boldsymbol{L}$ 과 $\boldsymbol{U}$ 는 대각성분이 모두 $0$ 인 하삼각행렬과 상삼각행렬이다. 이 때 $\boldsymbol{Ax}=\boldsymbol{b}$ 는 $(\boldsymbol{D} - \boldsymbol{L} -\boldsymbol{U})\boldsymbol{x}=\boldsymbol{b}$ 가 되며,

$$
\boldsymbol{Dx} = (\boldsymbol{L}+\boldsymbol{U})\boldsymbol{x}+\boldsymbol{b}
$$

이다. 모든 대각 성분이 $0$ 이 아닌 대각행렬 $\boldsymbol{D}$ 는 가역행렬이므로, 

$$
\boldsymbol{x} = \boldsymbol{D}^{-1}(\boldsymbol{L}+\boldsymbol{U})\boldsymbol{x} + \boldsymbol{D}^{-1}\boldsymbol{b}
$$

이다. 

임의로 제시하는 초기벡터 $\boldsymbol{x}^{(0)}$ 가 주어지면, 

$$
\boldsymbol{x}^{(k+1)} =   \boldsymbol{D}^{-1}(\boldsymbol{L}+\boldsymbol{U})\boldsymbol{x}^{(k)} + \boldsymbol{D}^{-1}\boldsymbol{b}
$$

를 통해 반복한다. Julia 코드는 다음과 같다.

```{.julia code-line-numbers="true"}
using LinearAlgebra

function iteration_jacobi(
    A::AbstractMatrix, 
    b::Vector, 
    x0::Vector; 
    etol::Number = 1.0e-5,
    Maxiter = 100_000)
    @assert size(A)[1] == size(A)[2] == size(b)[1]
    
    x = similar(x0)

    D = Diagonal(A)
    L = -LowerTriangular(A) .+ D
    U = -UpperTriangular(A) .+ D

    Dinv = inv(D)
    T = Dinv*(L+U) 
    c = Dinv * b
    for i in 1:Maxiter
        x = T*x0 + c
        if norm(x .- x0, Inf)/norm(x, Inf)< etol
            nitter = i
            println(nitter)
            return x
        else 
            x0 = x
        end
    end
    return nothing
end
```

</br>


$\boldsymbol{A} = \left[ \,\begin{array}{rr} 10 & -1 & 2 & 0\\ -1 & 11 & -1& 3\\ 2 & -1 & 10 &-1 \\ 0 & 3 & -1 & 8 \end{array}\,\right]$ 와 $\boldsymbol{b}= \left[ \begin{array}{rr}6 \\25 \\ -11 \\ 15 \end{array}\right]$ 에 대해 $\boldsymbol{Ax}=\boldsymbol{b}$ 의 해는 $\boldsymbol{x} = \left[\, \begin{array}{rr} 1 \\ 2 \\ -1 \\ 2 \end{array}\,\right]$ 이다. 

```julia
A = [10.0 -1.0 2.0 0.0; -1 11 -1 3; 2 -1 10 -1; 0 3 -1 8]
b = [6, 25, -11, 15]
x0 = [1, 1, -1, 1]

x=iteration_jacobi(A, b, x0, etol=1.0e-5)
```

를 이용하면 다음과 같은 결과를 얻는다.

```txt
4-element Vector{Float64}:
  0.9999964637124269
  2.0000051877531875
 -1.0000041868849727
  1.000006667932778
```

</br>

### 가우스-지델 방법

$(\boldsymbol{D}-\boldsymbol{L}-\boldsymbol{U})\boldsymbol{x} = \boldsymbol{b}$ 로 부터, $(\boldsymbol{D}-\boldsymbol{L})\boldsymbol{x} = \boldsymbol{U}\boldsymbol{x} + \boldsymbol{b}$ 로 변형 한 후

$$
\boldsymbol{x} =(\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{U}\boldsymbol{x} + (\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{b}
$$

를 얻는다. 가우스 지델 방법은

$$
\boldsymbol{x}^{(k+1)} = (\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{U}\boldsymbol{x}^{(k)} + (\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{b}
$$

를 통해 반복한다.

``` {.julia code-line-numbers="true"}
using LinearAlgebra
function iteration_gauss_siedel(
    A::AbstractMatrix, 
    b::Vector, 
    x0::Vector; 
    etol::Number = 1.0e-5, 
    Maxiter = 100_000)
    @assert size(A)[1] == size(A)[2] == size(b)[1]
    
    x = similar(x0)

    D = Diagonal(A)
    L = -LowerTriangular(A) .+ D
    U = -UpperTriangular(A) .+ D

    DLinv = inv(D-L)
    T = DLinv*U 
    c = DLinv * b
    for i in 1:Maxiter
        x = T*x0 + c
        if norm(x .- x0, Inf)/norm(x, Inf)< etol
            nitter = i
            println(nitter)
            return x
        else 
            x0 = x
        end
    end
    return nothing
end
```


야코비 방법과 가우스-지델 방법 모두 $\boldsymbol{x}^{(k+1)} = \boldsymbol{Tx}^{(k)} + \boldsymbol{c}$ 형태의 반복법을 사용한다. 우리는 @thm-iterative_method 로 부터 $\rho(\boldsymbol{T})<1$ 일 경우에만 이 반복법에 의해 답을 얻을 수 있다는 것을 안다. 일반적으로 가우스-지델 방법이 야코비 방법보다 빠르게 원하는 정확도의 답을 얻지만 가우스-지델 방법이 이 조건을 충족하지 못하고 야코비 방법은 충족하는 경우 야코비 방법을 사용 할 수 있다. 물론 거꾸로 야코비 방법이 이 조건을 충족하지 못하고 가우스-지델 방법이 충족할 수 도 있다. 

</br>

::: {#exm-spectral_radius}
$\boldsymbol{A} = \left[\, \begin{array}{rr} 2 & -1 & 1 \\ 2 & 2 & 2 \\ -1 & -1 & 2\end{array}\,\right]$ 의 경우 $\rho(\boldsymbol{D}^{-1}(\boldsymbol{L}+ \boldsymbol{U}))=\dfrac{\sqrt{5}}{2}>1$ 이므로 야코비 방법으로는 답을 얻을 수 없지만. $\rho((\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{U}) = \dfrac{1}{2}<1$ 이므로 가우스-지델 방법은 사용할 수 있다. 반대로 $\boldsymbol{A} = \left[\, \begin{array}{rr} 1 & 2 & -2 \\ 1 & 1 & 1 \\ 2 & 2 & 1\end{array}\,\right]$ 의 경우 $\rho(\boldsymbol{D}^{-1}(\boldsymbol{L} + \boldsymbol{U})) = 0<1$ 이므로 야코비 방법을 사용 할 수 있지만 $\rho((\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{U}) = 2>1$ 이므로 가우스-지델 방법은 사용 할 수 없다.

:::

</br>

## Successive Over-Relaxation (SOR)

가우스 지델 방법에서 임의의 $\omega\in \mathbb{R}$ 에 대해

$$
(\boldsymbol{D}-\omega \boldsymbol{L})\boldsymbol{x} = \left[(1-\omega) \boldsymbol{D}+ \omega\boldsymbol{U}\right]\boldsymbol{x} + \omega \boldsymbol{b}
$$

임을 안다. $SOR$ 은 적절한 $\omega$ 를 선택하여 다음 식을 통해 반복법으로 $\boldsymbol{Ax}=\boldsymbol{b}$ 의 해를 구하는 것이다.

$$
\boldsymbol{x}^{(k+1)} = (\boldsymbol{D}-\omega \boldsymbol{L})^{-1}\left[(1-\omega) \boldsymbol{D}+ \omega\boldsymbol{U}\right]\boldsymbol{x}^{(k)} + \omega (\boldsymbol{D}-\omega \boldsymbol{L})^{-1} \boldsymbol{b}
$$


다음의 결과를 증명 없이 사용하겠다.

</br>

::: {#thm-kahan}
#### Kahan 의 정리

모든 대각 성분이 $0$ 이 아닌 정사각 행렬 $\boldsymbol{A}$ 에 대해 $\boldsymbol{T}_\omega = (\boldsymbol{D}-\omega{L})^{-1}\left[(1-\omega) \boldsymbol{D}+ \omega\boldsymbol{U}\right]$ 를 생각하자. 이 때 

$$
\rho (\boldsymbol{T}_\omega) \ge |\omega -1|
$$

이 성립한다.
:::


</br>

즉 $0 <\omega <2$ 는 해를 구할 수 있는 필요조건이다.

</br>

::: {#thm-ostrowski-reich}

#### Ostrowski-Reich 정리

$\boldsymbol{A}\in \mathcal{M}_n(\mathbb{R})$ 이 positive definite 이고 $0<\omega <2$ 이면 SOR 에서의 $\boldsymbol{x}^{(k)}$ 는 수렴한다.

:::

</br>

::: {#thm-positive_definite_tridiagonal}

야코비 방법에서의 $\boldsymbol{T}_J = \boldsymbol{D}^{-1}(\boldsymbol{L}_\boldsymbol{U})$ 와 가우스 지델 방법에서의 $\boldsymbol{T}_G = (\boldsymbol{D}-\boldsymbol{L})^{-1}\boldsymbol{U}$ 에 대해 $\boldsymbol{A}$ 가 positive definite 이며 삼중대각행렬라면 $\rho(\boldsymbol{T}_G) = \left[\boldsymbol{T}_J\right]^2  < 1$ 이며, SOR 에서의 최적의 $\omega$ 는

$$
\omega = \dfrac{1}{1+\sqrt{1-[\rho(\boldsymbol{T}_G)]}}
$$

이다. 이 경우 $\rho(\boldsymbol{T}_\omega) = \omega -1$ 이다.

:::


</br>

## 오차 

가역 행렬 $\boldsymbol{A}\in \mathcal{M}_{n\times n}\mathbb{R}$ 에 대해 $\boldsymbol{Ax}=\boldsymbol{b}$ 를 반복법으로 풀 때 우리는 어떤 오차 내의 계산값을 얻게 된다. 이 계산값을 $\overline{\boldsymbol{x}}$ 라고 할 때 $\boldsymbol{r} = \boldsymbol{b}-\boldsymbol{A}\overline{\boldsymbol{x}}$ 를 **오차 벡터(residual vector)** 라고 할 수 있다. 정해진 노름에 대해 $\|\boldsymbol{r}\|$ 이 작다면 $\|\boldsymbol{x}-\overline{\boldsymbol{x}}\|$ 가 작을 것이라고 예상 할 수 있지만 항상 그런것은 아니다. 

</br>

::: {#prp-condition_number}

가역행렬 $\boldsymbol{A}$ 에 대한 선형방정식 $\boldsymbol{Ax}=\boldsymbol{b}$ 의 해 $\boldsymbol{x}\in \mathcal{M}_n (\mathbb{R})$ 과 어떤 $\overline{\boldsymbol{x}} \in \mathcal{M}_n (\mathbb{R})$, 그리고 오차 벡터 $\boldsymbol{r}= \boldsymbol{b}-\boldsymbol{A}\overline{\boldsymbol{x}}$ 과 벡터공간에서의 자연스러운 노름 $\|\cdot \|$ 에 대해 다음이 성립한다.

$$
\|\boldsymbol{x}-\overline{\boldsymbol{x}}\| \le \|\boldsymbol{r}\| \cdot \left\| \boldsymbol{A}^{-1}\right\|.
$$

또한 $\boldsymbol{x}\ne 0,\, \boldsymbol{b}\ne 0$ 일 때 다음이 성립한다.

$$
\dfrac{\|\boldsymbol{x}-\overline{\boldsymbol{x}}\|}{\|\boldsymbol{x}\|} \le \|\boldsymbol{A}\| \cdot \|\boldsymbol{A}^{-1}\| \dfrac{\|\boldsymbol{r}\|}{\|\boldsymbol{b}\|}.
$$

:::

::: {.proof}

$\boldsymbol{A}$ 가 가역이며 [행렬의 노름의 정의](03_matrix_algebra.qmd#def-matrix_norm)에 의해 
$$
\|\boldsymbol{x}-\overline{\boldsymbol{x}}\| = \|\boldsymbol{A}^{-1}\boldsymbol{r}\| \le \|\boldsymbol{A}^{-1}\| \cdot \|\boldsymbol{r}\|
$$

이다. $\boldsymbol{b}=\boldsymbol{Ax}$ 이므로 $\|\boldsymbol{b}\| \le \|\boldsymbol{A}\| \cdot \|\boldsymbol{x}\|$ 이다. 이로부터 $1/\|\boldsymbol{x}\|\le \|\boldsymbol{A}\| /\|\boldsymbol{b}\|$ 이므로, 
$$
\dfrac{\|\boldsymbol{x}-\overline{\boldsymbol{x}}\|}{\|\boldsymbol{x}\|} \le \|\boldsymbol{A}\| \cdot \|\boldsymbol{A}^{-1}\| \dfrac{\|\boldsymbol{r}\|}{\|\boldsymbol{b}\|}.
$$
이다. $\square$ 

:::

</br>

::: {.callout-note appearance="simple" icon="false"}

::: {#def-conditional_number_of_invertible_matrix}

#### 조건수

가역행렬 $\boldsymbol{A}$ 와 자연스러운 노름 $\|\cdot \|$ 에 대해 

$$
\kappa (\boldsymbol{A}) = \|\boldsymbol{A}\| \cdot \|\boldsymbol{A}^{-1}\|
$$

를 $\boldsymbol{A}$ 의 노름 $\|\cdot\|$ 에 대한 **조건수 (conditional number)** 라고 한다.

:::
:::

</br>



$$
1 = \|\boldsymbol{I}\|  = \|\boldsymbol{AA}^{-1}\| \le \|\boldsymbol{A} \| \cdot \|\boldsymbol{A}^{-1}\| = \kappa(\boldsymbol{A})
$$ 

이다. 만약 $\kappa(\boldsymbol{A})\approx 1$ 이면 오차벡터의 작은 차이가 $\|\boldsymbol{x}-\overline{\boldsymbol{x}}\|$ 의 작은 차이를 의미하기 때문에 $\boldsymbol{A}$ 를 **좋은 조건(well-conditioned)** 이라고 한다. 반대로 $\kappa(\boldsymbol{A}) \gg 1$ 일 경우는 **나쁜 조건 (ill-conditioned)** 라고 한다.



</br>

## The Conjugate gradient 방법

$\boldsymbol{A}$ 가 positive definite 이며 내적이 유클리드 내적, 즉 $\langle \boldsymbol{x},\,\boldsymbol{y} \rangle = \boldsymbol{y}^T\boldsymbol{x}$ 라고 하자. $\boldsymbol{A}$ 가 positive definite 이므로 $\langle \boldsymbol{Ax},\,\boldsymbol{x} \rangle = \boldsymbol{x}^T\boldsymbol{Ax} > 0$ 이다. 이때 우리는 다음을 보일 수 있다.

</br>

::: {#prp-congjugate_gradient}

$\boldsymbol{A}\in \mathcal{M}_n (\mathbb{R})$ 가 positive definite 이고 $\boldsymbol{x}_0 \in \mathcal{M}_n(\mathbb{R})$ 일 때 다음은 동치이다.

&emsp; ($1$) $\boldsymbol{Ax}_0=\boldsymbol{b}$,

&emsp; ($2$) $g(\boldsymbol{x})= \langle \boldsymbol{Ax},\,\boldsymbol{x}\rangle -2 \langle \boldsymbol{b}, \boldsymbol{x}\rangle$ 의 최소값은 $g(\boldsymbol{x}_0)$ 이다.
:::


::: {.proof}

우선 $t\in \mathbb{R},\, \boldsymbol{v}\in \mathcal{M}_n(\mathbb{R})$ 에 대해, 

$$
\begin{aligned}
g(\boldsymbol{x}+t\boldsymbol{v}) &= \langle \boldsymbol{A}(\boldsymbol{x}+t\boldsymbol{v}), \, \boldsymbol{x}+t\boldsymbol{v} \rangle - 2\langle \boldsymbol{b},\, \boldsymbol{x}+t\boldsymbol{v}\rangle  \\
&= t^2 \langle \boldsymbol{Av}, \boldsymbol{v}\rangle - 2t \langle \boldsymbol{b}-\boldsymbol{Ax}, \, \boldsymbol{v}\rangle + g(\boldsymbol{x}) \\
&= \langle \boldsymbol{Av},\,\boldsymbol{v} \rangle \left[t-\dfrac{\langle \boldsymbol{b}-\boldsymbol{Ax}, \, \boldsymbol{v}\rangle }{\langle \boldsymbol{Av},\, \boldsymbol{v}\rangle}\right]^2 + g(\boldsymbol{x}) - \dfrac{\langle \boldsymbol{b}-\boldsymbol{Ax}, \, \boldsymbol{v}\rangle^2 }{\langle \boldsymbol{Av},\, \boldsymbol{v}\rangle}
\end{aligned}
$$ {#eq-conjugate_gradient}

이다. $\boldsymbol{A}$ 가 positive definite 이므로 $\langle \boldsymbol{Av}, \,\boldsymbol{v}\rangle > 0$ 이다.

$\boldsymbol{Ax_0}=\boldsymbol{b}$ 이면 $g(\boldsymbol{x_0}+t\boldsymbol{v}) \ge g(\boldsymbol{x_0})$ 이므로 $g(\boldsymbol{x})$ 의 최소값이다. 역으로 $g(\boldsymbol{x})$ 를 최소로 하는 $\boldsymbol{x}$ 는 $\boldsymbol{b}= \boldsymbol{Ax}_0$ 를 만족하는 $\boldsymbol{x}_0$ 임을 알 수 있다. $\square$


:::

</br>

이제 $\boldsymbol{Ax}=\boldsymbol{b}$ 라는 선형 시스템을 푸는 문제는 $g(\boldsymbol{x})= \langle \boldsymbol{Ax},\,\boldsymbol{x}\rangle -2 \langle \boldsymbol{b}, \boldsymbol{x}\rangle$ 를 최소로 하는 $\boldsymbol{x}_0$ 를 구하는 문제와 동일한 문제가 된다. 주어진 $\boldsymbol{x}^{(k)}$ 를 생각하자. $g(\boldsymbol{x}^{(k)} + t_k \boldsymbol{v}^{(k)}) \le g(\boldsymbol{x}^{(k)})$ 가 되도록 $t_k$ 와 $\boldsymbol{v}^{(k)}$ 를 정할 수 있다면 우리는 점차 $g(\boldsymbol{x})$ 의 최소값에 다가갈 것이며, 결국은 $g(\boldsymbol{x})$ 를 최소로 하는 $\boldsymbol{x}_0$ 를 얻게 되고 이것은 $\boldsymbol{Ax}=\boldsymbol{b}$ 의 해를 구하는 것이 된다.

</br>


우선 $t_k$ 와 $\boldsymbol{v}^{(k)}$ 를 결정하는 방법은 두가지가 있다

### Steepest descent


우리는 다변수를 다루는 미적분학 혹은 해석학에서 $g :\mathbb{R}^n \to \mathbb{R}$ 가 $C^1$ 급 함수일 때 $g(\boldsymbol{x})$ 를 증가시키는 방향이 $\nabla g = (\partial_1 g,\ldots,\, \partial_n g)$ 임을 배웠다. $\boldsymbol{x} = \begin{bmatrix} x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 

$$
g(\boldsymbol{x}) = \sum_{i, j=1}^n A_{ij}x_i x_j - 2 \sum_{i=1}^n b_i x_i
$$

이므로, 

$$
\partial_i g(\boldsymbol{x}) = 2(\boldsymbol{Ax})_i -2b_i
$$

이다. 즉,

$$
\nabla g(\boldsymbol{x}) = 2 \boldsymbol{Ax}-2\boldsymbol{b}
$$

이다.  우리는 $g(\boldsymbol{x})$ 를 최소로 하고자 하므로 $\boldsymbol{v}^{(k)}$ 를 $-\nabla g(\boldsymbol{x})$ 방향으로 잡아야 한다. 즉 $\boldsymbol{v}^{(k)}= \boldsymbol{b}-\boldsymbol{Ax}^{(k)}$ 으로 잡을 수 있다. 

@eq-conjugate_gradient 에 의해 정해진 $\boldsymbol{x}^{(k)}$ 와 $\boldsymbol{v}^{(k)}$ 로 $g(\boldsymbol{x}^{(k)}+t\boldsymbol{v}^{(k)})$ 를 최소화 하는 $t$ 는 

$$
t_k = \dfrac{\langle \boldsymbol{b}-\boldsymbol{Ax}^{(k)}, \, \boldsymbol{v}^{(k)}\rangle }{\langle \boldsymbol{Av}^{(k)},\, \boldsymbol{v}^{(k)}\rangle}
$$

이다. 이것을 Julia 코드로 구현한 것은 아래와 같다. 위의 방법과 다른 것은 반복이 멈추는 것을 $\|\boldsymbol{b}-\boldsymbol{Ax}^{(k)}\|_\infty$ 가 `etol` 이라는 함수 인자보다 작을 때로 하였다는 것이다.


``` {.julia code-line-numbers="true"}
function steepest_iteration(
    A::AbstractMatrix, 
    b::Vector, 
    x0::Vector;
    etol::Number = 1.0e-5, 
    Maxiter = 100_000)

    x = similar(x0)
    for i in 1:Maxiter
        v = b - A*x0
        t = dot(v,(b-A*x0))/dot(v, (A*v))
        x = x0 + t*v
        if norm(A*x-b, Inf)<etol
            nitter = i
            return x
        else 
            x0 = x
        end
    end
    return nothing
end
```



</br>

### $\boldsymbol{A}$-직교 조건

영벡터가 아닌 벡터의 집합 $\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(m)}\} \subset \mathcal{M}_{n}(\mathbb{R})$ 이 $i \ne j \implies \langle \boldsymbol{Av}^{(i)},\, \boldsymbol{v}^{(j)}\rangle = 0$ 일 때 $\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(m)}\}$ 를 $\boldsymbol{A}$-직교 벡터라고 한다. 

</br>

::: {#prp-A_orthogonal_vectors_are_linearly_independent}

Positive definite $\boldsymbol{A}$ 에 대해 $\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(m)}\}$ 가 $\boldsymbol{A}$-직교 벡터일 때 이 벡터들은 선형독립이다.

:::

::: {.proof}

$\sum_i c_i \boldsymbol{v}^{(i)} = \boldsymbol{0}$ 일 때 임의의 $\boldsymbol{v}^{(j)}$ 에 대해 

$$
0 = \left\langle \boldsymbol{A} \left( \sum_i c_i \boldsymbol{v}^{(i)} \right), \, \boldsymbol{v}^{(j)}\right\rangle = c_j
$$ 

이다. 따라서 $c_1 = \cdots = c_m=0$ 이므로 $\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(m)}\}$ 는 선형독립이다.

:::

</br>

즉 $\boldsymbol{A}\in \mathcal{M}_{n\times n}(\mathbb{R})$ 에 대해 $\boldsymbol{A}$-직교인 $n$ 개의 벡터를 얻었다면 이 $n$ 개의 벡터는 $\mathcal{M}_n(\mathbb{R})$ 의 기저가 된다. 이제 $\boldsymbol{A}$-직교인 $\{\boldsymbol{v}^{(k)} : k=1,\ldots,\,n\}$ 를 구해야 할 것 같지만 잠시 미뤄두자. 구하는 것은 복잡하거나 어렵지 않다. 일단 $\boldsymbol{A}$-직교인 $\{\boldsymbol{v}^{(k)} : k=1,\ldots,\,n\}$ 를 안다면 우리는 최대 $n$ 번의 iteration 선형방정식의 해를 이론적으로 구할 수 있다는 것을 보일 수 있다. 여기서 이론적이라는 것은 roundoff-error 등으로 인한 오차를 생각하지 않는다는 조건이다.

</br>

::: {#thm-A-orthogonal_method}

Positive definite $\boldsymbol{A}\in \mathcal{M}_{n \times n}(\mathbb{R})$ 에 대해 $\{\boldsymbol{v}^{(1)}, \ldots,\,\boldsymbol{v}^{(n)}\}$ 이 $\boldsymbol{A}$-직교 벡터라고 하자. 임의의 $\boldsymbol{x}_1\in \mathcal{M}$ 과 $k=1,\ldots,\,n$ 에 대해 

$$
\begin{aligned}
t_k &= \dfrac{\langle \boldsymbol{b}-\boldsymbol{Ax}^{(k)}, \, \boldsymbol{v}^{(k)}\rangle }{\langle \boldsymbol{Av}^{(k)},\, \boldsymbol{v}^{(k)}\rangle}, \\
\boldsymbol{x}^{(k+1)} &= \boldsymbol{x}^{(k)} + t_k \boldsymbol{v}^{(k)}
\end{aligned}
$$


라면, $\boldsymbol{Ax}^{(k+1)} = \boldsymbol{b}$ 이다.


:::

::: {.proof}

주어진 식으로부터
$$
\begin{aligned}
\boldsymbol{Ax}^{(n+1)} &= \boldsymbol{Ax}^{(n)} + t_ n \boldsymbol{Av}^{(n)} \\
&= \boldsymbol{A}(\boldsymbol{x}^{(n-1)}  + t_{n-1}\boldsymbol{v}^{(n-1)}) + t_n \boldsymbol{Av}^{(n)}\\
& \qquad \qquad  \vdots \\
& = \boldsymbol{Ax}^{(1)} + t_1\boldsymbol{Av}^{(1)} + \cdots + t_n \boldsymbol{Av}^{(n)} 
\end{aligned}
$$ {#eq-positive_definite_1}

을 얻는다. 따라서,

$$
\boldsymbol{Ax}^{(n+1)} - \boldsymbol{b} = (\boldsymbol{Ax}^{(1)} - \boldsymbol{b})+ t_1\boldsymbol{Av}^{(1)} + \cdots + t_n \boldsymbol{Av}^{(n)}
$$

이다. 여기에 $\boldsymbol{v}^{(k)}$ 와의 내적을 구하면

$$
\langle \boldsymbol{Ax}^{(n+1)} - \boldsymbol{b},\, \boldsymbol{v}^{(k)}\rangle = \langle \boldsymbol{Ax}^{(1)}- \boldsymbol{b}, \, \boldsymbol{v}^{(k)}\rangle + t_k \langle \boldsymbol{Av}^{(k)},\, \, \boldsymbol{v}^{(k)}\rangle
$$

이며 $t_k$ 의 정의로부터 

$$
t_k \langle \boldsymbol{Av}^{(k)},\, \, \boldsymbol{v}^{(k)}\rangle = \langle \boldsymbol{b}-\boldsymbol{Ax}^{(k)}, \, \boldsymbol{v}^{(k)}\rangle
$$

이므로, 

$$
\langle \boldsymbol{Ax}^{(n+1)}-\boldsymbol{b},\, \boldsymbol{v}^{(k)}\rangle = \langle \boldsymbol{A}(\boldsymbol{x}^{(1)}-\boldsymbol{x}^{(k)})-\boldsymbol{b}, \, \boldsymbol{v}^{(k)}\rangle
$$

임을 얻을 수 있다. @eq-positive_definite_1 로부터, $\boldsymbol{Ax}^{(k)}$ 는 $\boldsymbol{Ax}^{(1)}$ 과 $\boldsymbol{Av}^{(1)}, \ldots, \boldsymbol{Av}^{(k-1)}$ 의 선형결합임을 안다. 즉 위식의 우변은 $0$ 이다. 따라서

$$
\boldsymbol{Ax}^{(n+1)} = \boldsymbol{b}
$$

이다. $\square$ 

:::


</br>

이제 $\boldsymbol{A}$-직교 벡터를 구하는 방법을 알아볼 차례이다. 다음을 보자.

::: {#thm-A_orthogonal_vectors_1}

Positive definite $\boldsymbol{A}\in \mathcal{M}_{n \times n}(\mathbb{R})$ 에 대해 $\boldsymbol{r}^{(k)}=\boldsymbol{b}-\boldsymbol{Ax}^{(k)}$ 이라 하면 $j=1,\ldots,\,k-1$ 에 대해 $\langle \boldsymbol{r}^{(k)}, \boldsymbol{v}^{(j)}\rangle=0$ 이다.

:::

::: {.proof}

Induction 으로 증명한다. $k=2$ 에 대해 $\boldsymbol{r}^{(2)} = \boldsymbol{b}-\boldsymbol{Ax}^{(2)}$ 이며 $\boldsymbol{x}^{(2)} =  \boldsymbol{x}^{(1)} +t_1 \boldsymbol{v}^{(1)}$ 이다.  $t_1 = \dfrac{\langle \boldsymbol{b}-\boldsymbol{Ax}^{(1)}, \boldsymbol{v}^{(1)}\rangle}{\langle \boldsymbol{Av}^{(1)},\boldsymbol{v}^{(1)}\rangle}$ 이므로, 

$$
\begin{aligned}
\langle \boldsymbol{r}^{(2)},\boldsymbol{v}^{(1)}\rangle &= \langle \boldsymbol{b}-\boldsymbol{Ax}^{(1)} - t_1\boldsymbol{Av}^{(1)},\boldsymbol{v}^{(1)}\rangle \\
&= \langle \boldsymbol{b}-\boldsymbol{Ax}^{(1)},\boldsymbol{v}^{(1)}\rangle - t_1 \langle \boldsymbol{Av}^{(1)},\boldsymbol{v}^{(1)}\rangle = 0
\end{aligned}
$$

이다. 이제 $k<n$ 에 대해 성립함을 가정하자. $j=1,\ldots,\,k$ 에 대해 다음이 성립함을 보이면 된다.

$$
\begin{aligned}
\langle \boldsymbol{r}^{(k+1)}, \boldsymbol{v}^{(j)}\rangle &= \langle \boldsymbol{b}-\boldsymbol{Ax}^{(k+1)},\,\boldsymbol{v}^{(j)}\rangle \\
&= \langle \boldsymbol{b} - \boldsymbol{Ax}^{(k)} - t_k\boldsymbol{Av}^{(k)},\, \boldsymbol{v}^{(j)}\rangle \\
&= \langle \boldsymbol{r}^{(k)}, \boldsymbol{v}^{(j)}\rangle - t_k \langle \boldsymbol{Av}^{(k)}, \boldsymbol{v}^{(j)}\rangle
\end{aligned}
$$

이다. $j=<k$ 이면 induction 의 가정에 의해 $\langle \boldsymbol{r}^{(k)}, \boldsymbol{v}^{(j)}\rangle =0$ 이며 $\boldsymbol{A}$-직교 벡터의 정의에 의해 $\langle \boldsymbol{Av}^{(k)}, \boldsymbol{v}^{(j)}\rangle=0$ 이므로 성립한다. $j=k$ 일 때는 $t_k$ 의 정의에 의해 $0$ 이 된다. $\square$

:::

</br>

::: {#thm-A_orthogonal_vectors_2}

아래와 같이 정의된 $\boldsymbol{v}^{(k)}$ $(k\ge 2)$ 는 $j=1,\,\ldots,k-1$ 에 대해 $\langle \boldsymbol{v}^{(k)},\, \boldsymbol{Av}^{(j)}\rangle = 0$ 이다.

$$
\boldsymbol{v}^{(k)}=\boldsymbol{r}^{(k)} + s_k \boldsymbol{v}^{(k-1)}, \qquad \text{where } s_k = \dfrac{\langle \boldsymbol{r}^{(k)}, \,\boldsymbol{r}^{(k)} \rangle}{\langle \boldsymbol{r}^{(k-1)}, \,\boldsymbol{r}^{(k-1)} \rangle} 
$$

:::

::: {.proof}

$k=2$ 일 때 

$$
\begin{aligned}
\boldsymbol{v}^{(2)} &= \boldsymbol{b}-\boldsymbol{Ax}^{(2)} + s_1 \boldsymbol{v}^{(1)} \\
&= \boldsymbol{b}-\boldsymbol{A}(\boldsymbol{x}^{(1)}-t_1 \boldsymbol{v}^{(1)})+ s_1 \boldsymbol{v}^{(1)}\\
&= \boldsymbol{b} - \boldsymbol{Ax}^{(1)} + 
\end{aligned}
$$ 


:::
